[
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Course Materials",
    "section": "",
    "text": "Practical 1\n\nSlides\n\nPractical 2\nPractical 3\nPractical 4\nPractical 5\nPractical 6\n\nDatasets",
    "crumbs": [
      "Materials"
    ]
  },
  {
    "objectID": "slides/intro.html#who-transport-data-science-team",
    "href": "slides/intro.html#who-transport-data-science-team",
    "title": "Introduction to transport data science",
    "section": "Who: Transport Data Science team",
    "text": "Who: Transport Data Science team\nRobin Lovelace\n\nAssociate Professor of Transport Data Science\nResearching transport futures and active travel planning\nR developer and teacher, author of Geocomputation with R\n\nYuanxuan Yang\n\nLecturer in Data Science of Transport\nNew and Emerging Forms of Data: Investigating novel data sources and their applications in urban mobility and transport planning."
  },
  {
    "objectID": "slides/intro.html#tds-team-ii",
    "href": "slides/intro.html#tds-team-ii",
    "title": "Introduction to transport data science",
    "section": "TDS Team II",
    "text": "TDS Team II\nMalcolm Morgan\n\nSenior researcher at ITS with expertise in routing + web\nDeveloper of the Propensity to Cycle Tool and PBCC\n\nZhao Wang\n\nCivil Engineer and Data Scientist with expertise in machine learning\n\nDemonstrators\n\nJuan Pablo Fonseca Zamora\n\nYou!"
  },
  {
    "objectID": "slides/intro.html#what-is-transport-data-science",
    "href": "slides/intro.html#what-is-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "What is transport data science?",
    "text": "What is transport data science?\n\n\nThe application of data science to transport datasets and problems\nRaising the question‚Ä¶\nWhat is data science?\nA discipline ‚Äúthat allows you to turn raw data into understanding, insight, and knowledge‚Äù (Grolemund, 2016)\n\nIn other words‚Ä¶\n\nStatistics that is actually useful!"
  },
  {
    "objectID": "slides/intro.html#why-take-transport-data-science",
    "href": "slides/intro.html#why-take-transport-data-science",
    "title": "Introduction to transport data science",
    "section": "Why take Transport Data Science",
    "text": "Why take Transport Data Science\n\n\n\n\nNew skills (cutting edge R and/or Python packages)\nPotential for impacts\nAllows you to do new things with data\nIt might get you a job!"
  },
  {
    "objectID": "slides/intro.html#live-demo-npt.scot-web-app",
    "href": "slides/intro.html#live-demo-npt.scot-web-app",
    "title": "Introduction to transport data science",
    "section": "Live demo: npt.scot web app",
    "text": "Live demo: npt.scot web app"
  },
  {
    "objectID": "slides/intro.html#the-history-of-tds",
    "href": "slides/intro.html#the-history-of-tds",
    "title": "Introduction to transport data science",
    "section": "The history of TDS",
    "text": "The history of TDS\n\n2017: Transport Data Science created, led by Dr Charles Fox, Computer Scientist, author of Transport Data Science book (Fox, 2018)\nThe focus was on databases and Bayesian methods\n2019: I inherited the module, which was attended by ITS students\nSummer 2019: Python code published in the module ‚Äòrepo‚Äô:\n\ngithub.com/ITSLeeds"
  },
  {
    "objectID": "slides/intro.html#history-of-tds-ii",
    "href": "slides/intro.html#history-of-tds-ii",
    "title": "Introduction to transport data science",
    "section": "History of TDS II",
    "text": "History of TDS II\n\nJanuary 2020: Available, Data Science MSc course\nMarch 2020: Switch to online teaching\n2021-2023: Updated module, focus on methods\n2024: Switch to combined practical sessions and lectures\n2025+: Expand, online course? book? stay in touch!\n\n\n\nMilestone passed in my academic career, first online-only delivery of lecture @ITSLeeds, seems to have worked, live code demo with #rstats/@rstudio, recording, chat + allüéâThanks students for ‚Äòattending‚Äô + remote participation, we‚Äôll get through this together.#coronavirus pic.twitter.com/wlAUxmZj5r\n\n‚Äî Robin Lovelace (@robinlovelace) March 17, 2020"
  },
  {
    "objectID": "slides/intro.html#essential-reading",
    "href": "slides/intro.html#essential-reading",
    "title": "Introduction to transport data science",
    "section": "Essential reading",
    "text": "Essential reading\n\nChapter 12, Transportation of Geocomputation with R, a open book on geographic data in R (available free online) (Lovelace et al.¬†2019)\nReproducible Road Safety Research with R (RRSRR): https://itsleeds.github.io/rrsrr/"
  },
  {
    "objectID": "slides/intro.html#core-reading-materials",
    "href": "slides/intro.html#core-reading-materials",
    "title": "Introduction to transport data science",
    "section": "Core reading materials",
    "text": "Core reading materials\n\nR for Data Science, an introduction to data science with R (available free online)\nPython equivalent"
  },
  {
    "objectID": "slides/intro.html#optional",
    "href": "slides/intro.html#optional",
    "title": "Introduction to transport data science",
    "section": "Optional",
    "text": "Optional\nThere are many good resources on data science for transport applications. Do your own research and reading! The following are good:\n\nIf you‚Äôre interested in network analysis/Python, see this paper on analysing OSM data in Python (Boeing and Waddell, 2017) (available online)\nIf you‚Äôre interested in the range of transport modelling tools, see Lovelace (2021). \n\nFor more references, see the bibliography at github.com/ITSLeeds/TDS"
  },
  {
    "objectID": "slides/intro.html#objectives",
    "href": "slides/intro.html#objectives",
    "title": "Introduction to transport data science",
    "section": "Objectives",
    "text": "Objectives\n\n\nUnderstand the structure of transport datasets\nUnderstand how to obtain, clean and store transport related data\nGain proficiency in command-line tools for handling large transport datasets\nProduce data visualizations, static and interactive\n Learn how to join together the components of transport data science into a cohesive project portfolio"
  },
  {
    "objectID": "slides/intro.html#assessment-for-those-doing-this-as-credit-bearing",
    "href": "slides/intro.html#assessment-for-those-doing-this-as-credit-bearing",
    "title": "Introduction to transport data science",
    "section": "Assessment (for those doing this as credit-bearing)",
    "text": "Assessment (for those doing this as credit-bearing)\n\nYou will build-up a portfolio of work\n100% coursework assessed, you will submit by\nWritten in code - will be graded for reproducibility\nCode chunks and figures are encouraged\nYou will submit a non-assessed 2 page pdf + qmd"
  },
  {
    "objectID": "slides/intro.html#schedule",
    "href": "slides/intro.html#schedule",
    "title": "Introduction to transport data science",
    "section": "Schedule",
    "text": "Schedule"
  },
  {
    "objectID": "slides/intro.html#feedback",
    "href": "slides/intro.html#feedback",
    "title": "Introduction to transport data science",
    "section": "Feedback",
    "text": "Feedback\n\nThe module is taught by two really well organised and enthusiastic professors, great module, the seminars, structured and unstructured learning was great and well thought out, all came together well\n\n\nI wish this module was 60 credits instead of 15 because i just want more of it."
  },
  {
    "objectID": "p1/index.html",
    "href": "p1/index.html",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "",
    "text": "How do you see yourself using data science over the next 5 years?\nThink of a question about a transport system you know well and how data science could help answer it, perhaps with reference to a sketch like that below\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://www.openstreetmap.org/#map=19/53.80689/-1.55637 for more ideas"
  },
  {
    "objectID": "p1/index.html#thinking-about-transport-data-science",
    "href": "p1/index.html#thinking-about-transport-data-science",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "",
    "text": "How do you see yourself using data science over the next 5 years?\nThink of a question about a transport system you know well and how data science could help answer it, perhaps with reference to a sketch like that below\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee https://www.openstreetmap.org/#map=19/53.80689/-1.55637 for more ideas"
  },
  {
    "objectID": "p1/index.html#bonus-reproduce-the-contents-of-chapter-2-in-python",
    "href": "p1/index.html#bonus-reproduce-the-contents-of-chapter-2-in-python",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "1.1 Bonus: reproduce the contents of Chapter 2 in Python",
    "text": "1.1 Bonus: reproduce the contents of Chapter 2 in Python"
  },
  {
    "objectID": "p1/index.html#completing-chapters-2-4-in-the-rrsrr-document",
    "href": "p1/index.html#completing-chapters-2-4-in-the-rrsrr-document",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "2.1 Completing Chapters 2-4 in the RRSRR document",
    "text": "2.1 Completing Chapters 2-4 in the RRSRR document\n\nThis was your homework but revisit it an ensure you understand every bit and complete it if you haven‚Äôt already"
  },
  {
    "objectID": "p1/index.html#tidyverse",
    "href": "p1/index.html#tidyverse",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "2.2 Tidyverse",
    "text": "2.2 Tidyverse\n\nWork through Chapter 5 starting with the following code:\n\n\nlibrary(tidyverse) # Load the package\n\n‚îÄ‚îÄ Attaching core tidyverse packages ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse 2.0.0 ‚îÄ‚îÄ\n‚úî dplyr     1.1.4     ‚úî readr     2.1.5\n‚úî forcats   1.0.0     ‚úî stringr   1.5.1\n‚úî ggplot2   3.5.1     ‚úî tibble    3.2.1\n‚úî lubridate 1.9.3     ‚úî tidyr     1.3.1\n‚úî purrr     1.0.2     \n‚îÄ‚îÄ Conflicts ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ tidyverse_conflicts() ‚îÄ‚îÄ\n‚úñ dplyr::filter() masks stats::filter()\n‚úñ dplyr::lag()    masks stats::lag()\n‚Ñπ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors"
  },
  {
    "objectID": "p1/index.html#bonus-analysis-of-flights-data",
    "href": "p1/index.html#bonus-analysis-of-flights-data",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "3.1 Bonus: Analysis of flights data",
    "text": "3.1 Bonus: Analysis of flights data\nYou need to have a number of packages installed and loaded. Install the packages by typing in the following commands into RStudio (you do not need to add the comments after the # symbol):1\n\ninstall.packages(\"remotes\")\n\n\npkgs = c(\n  \"nycflights13\",# data package\n  \"stats19\",     # downloads and formats open stats19 crash data\n  \"tidyverse\"   # a package for user friendly data science\n)\nremotes::install_cran(pkgs)\n\nSkipping install of 'nycflights13' from a cran remote, the SHA1 (1.0.2) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\nSkipping install of 'stats19' from a cran remote, the SHA1 (3.1.0) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\nSkipping install of 'tidyverse' from a cran remote, the SHA1 (2.0.0) has not changed since last install.\n  Use `force = TRUE` to force installation\n\nremotes::install_github(\"nowosad/spDataLarge\")\n\nUsing github PAT from envvar GITHUB_TOKEN. Use `gitcreds::gitcreds_set()` and unset GITHUB_TOKEN in .Renviron (or elsewhere) if you want to use the more secure git credential store instead.\n\n\nSkipping install of 'spDataLarge' from a github remote, the SHA1 (4c2d08a9) has not changed since last install.\n  Use `force = TRUE` to force installation\n\n\nLoad the tidyverse package as follows:\n\nlibrary(tidyverse)\n\nThis section will use content from Chapter 5 of the R for Data Science book (grolemund_data_2016?).\n\nRead section 5.1 of R for Data Science and write code that reproduces the results in that section in the script learning-tidyverse.R\n\nYour script will start with something like this:\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\n\nTake a random sample of 10,000 flights and assign it to an object with the following line of code:\n\n\nlibrary(nycflights13)\nflights_sample = sample_n(flights, 1e4)\nunique(flights$carrier)\n\n [1] \"UA\" \"AA\" \"B6\" \"DL\" \"EV\" \"MQ\" \"US\" \"WN\" \"VX\" \"FL\" \"AS\" \"9E\" \"F9\" \"HA\" \"YV\"\n[16] \"OO\"\n\n\n\nFind the unique carriers with the unique() function\nCreate an object containing flights from United, American, or Delta, and assign it to f, as follows:\n\n\nf = filter(flights, grepl(pattern = \"UA|AA|DL\", x = carrier))\nf2 = filter(flights, grepl(pattern = \"UA\", x = carrier) |\n             grepl(pattern = \"AA\", x = carrier) |\n             grepl(pattern = \"DL\", x = carrier)\n           )\nf3 = filter(flights, str_detect(carrier, \"UA|AA|DL\"))\n\n\nCreate plots that visualise the sample flights, using code from Chapter 3 of the same book, starting with the following plot:\n\n\nggplot(f) +\n  geom_point(aes(air_time, distance))\n\n\n\n\n\n\n\n\n\nAdd transparency so it looks like this (hint: use alpha = in the geom_point() function call):\n\n\n\nWarning: Removed 2117 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nAdd a colour for each carrier, so it looks something like this:\n\n\nggplot(f) +\n  geom_point(aes(air_time, distance, colour = carrier), alpha = 0.5)\n\nWarning: Removed 2117 rows containing missing values or values outside the scale range\n(`geom_point()`).\n\n\n\n\n\n\n\n\n\n\nBonus 1: find the average air time of those flights with a distance of 1000 to 2000 miles\nBonus 2: use the lm() function to find the relationship between flight distance and time, and plot the results (start the plot as follows, why did we use na.omit()? hint - find help with ?na.omit()):\n\n\nf = na.omit(f)\nm = lm(air_time ~ distance, data = f)\nf$pred = m$fitted.values"
  },
  {
    "objectID": "p1/index.html#footnotes",
    "href": "p1/index.html#footnotes",
    "title": "Practical 1: Introduction to Transport Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n Note: if you want to install the development version of a package from GitHub, you can do so. Try, for example, running the following command: remotes::install_github(\"ITSLeeds/pct\")‚Ü©Ô∏é"
  },
  {
    "objectID": "marking-criteria.html",
    "href": "marking-criteria.html",
    "title": "Marking Criteria",
    "section": "",
    "text": "Marks are awarded in 4 categories, accounting for the following criteria:\n\n\n\nThe selection and effective use of input datasets that are large (e.g.¬†covering multiple years), complex (e.g.¬†containing multiple variables) and/or diverse (e.g.¬†input datasets from multiple sources are used and where appropriate combined in the analysis)\nDescribe how the data was collected and implications for data quality, and outline how the input datasets were downloaded (with a reproducible example if possible), with a description that will allow others to understand the structure of the inputs and how to import them\nEvidence of data cleaning techniques (e.g.¬†by re-categorising variables)\nAdding value to datasets with joins (key-based or spatial), creation of new variables (also known as feature engineering) and reshaping data (e.g.¬†from wide to long format)\n\nDistinction (70%+): The report makes use of a complex (with many columns and rows) and/or multiple input datasets, efficiently importing them and adding value by creating new variables, recategorising, changing data formats/types, and/or reshaping the data. Selected datasets are very well suited to the research questions, clearly described, with links to the source and understanding of how the datasets were generated.\nMerit (60-69%): The report makes some use of complex or multiple input datasets. The selection, description of, cleaning or value-added to the input datasets show skill and care applied to the data processing stage but with some weaknesses. Selected datasets are appropriate for the research questions, with some description or links to the data source.\nPass (50-59%): There is some evidence of care and attention put into the selection, description of or cleaning of the input datasets but little value has been added. The report makes little use of complex or multiple input datasets. The datasets are not appropriate for the research questions, the datasets are not clearly described, or there are no links to the source or understanding of how the datasets were generated, but the data processing aspect of the work acceptable.\nFail (0-49%): The report does not make use of appropriate input datasets and contains very little or now evidence of data cleaning, adding value to the datasets or reshaping the data. While there may be some evidence of data processing, it is of poor quality and/or not appropriate for the research questions.\n\n\n\n\nCreation of figures that are readable and well-described (e.g.¬†with captions and description)\nHigh quality, attractive or advanced techniques (e.g.¬†multi-layered maps or graphs, facets or other advanced techniques)\nUsing visualisation techniques appropriate to the topic and data and interpreting the results correctly (e.g.¬†mentioning potential confounding factors that could account for observed patterns)\nThe report is well-formatted, accessible (e.g.¬†with legible text size and does not contain excessive code in the submitted report) and clearly communicates the data and analysis visually, with appropriate figure captions, cross-references and a consistent style\n\nDistinction (70%+): The report contains high quality, attractive, advanced and meaningful visualisations that are very well-described and interpreted, showing deep understanding of how visualisation can communicate meaning contained within datasets. The report is very well-formatted, accessible and clearly communicates the data and analysis visually.\nMerit (60-69%): The report contains good visualisations that correctly present the data and highlight key patterns. The report is has appropriate formatting.\nPass (50-59%): The report contains basic visualisations or are not well-described or interpreted correctly or the report is poorly formatted, not accessible or does not clearly communicate the data and analysis visually.\nFail (0-49%): The report is of unacceptable quality (would likely be rejected in a professional setting) and/or has poor quality and/or few visualisations, or the visualisations are inappropriate given the data and research questions.\n\n\n\n\nCode quality in the submitted source code, including using consistent style, appropriate packages, and clear comments\nEfficiency, including pre-processing to reduce input datasets (avoiding having to share large datasets in the submission for example) and computationally efficient implementations\nThe report is fully reproducible, including generation of figures. There are links to online resources for others wanting to reproduce the analysis for another area, and links to the input data\n\nDistinction (70%+): The source code underlying the report contains high quality, efficient and reproducible code that is very well-written, using consistent syntax and good style, well-commented and uses appropriate packages. The report is fully reproducible, with links to online resources for others wanting to reproduce the analysis for another area, and links to the input data.\nMerit (60-69%): The code is readable and describes the outputs in the report but lacks quality, either in terms of comments, efficiency or reproducibility.\nPass (50-59%): The source code underlying the report describes the outputs in the report but is not well-commented, not efficient or has very limited levels of reproduicibility, with few links to online resources for others wanting to reproduce the analysis for another area, and few links to the input data.\nFail (0-49%): The report has little to no reproducible, readable or efficient code. A report that includes limited well-described code in the main text or in associated files would be considered at the borderline between a fail and a pass. A report that includes no code would be considered a low fail under this criterion.\n\n\n\n\nTopic selection, including originality, availability of datasets related to the topic and relevance to solving transport planning problems\nClear research question\nAppropriate reference to the academic, policy and/or technical literature and use of the literature to inform the research question and methods\nUse of appropriate data science methods and techniques\nDiscussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed\nDiscuss further research and/or explain the potential impacts of the work\nThe conclusions are supported by the analysis and results\nThe contents of the report fit together logically and support the aims and/or research questions of the report\n\nDistinction (70%+): The report contains a clear research question, appropriate reference to the academic, policy and/or technical literature, use of appropriate data science methods and techniques, discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed. The report discusses further research and/or explores of the potential impacts of the work. Conclusions are supported by the analysis and results, and the contents of the report fit together logically as a cohehisive whole that has a clear direction set-out by the aims and/or research questions. To get a Distinction there should also be evidence of considering the generalisability of the methods and reflections on how it could be built on by others in other areas.\nMerit (60-69%): There is a clear research question. There is some reference to the academic, policy and/or technical literature. The report has a good structure and the results are supported by the analysis. There is some discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed.\nPass (50-59%): The report contains a valid research question but only limited references to appropriate literature or justification. There is evidence of awareness of the limitations of the results and how they inform conclusions, but these are not fully supported by the analysis. The report has a reasonable structure but does not fit together well in a cohesive whole.\nFail (0-49%): The report does not contain a valid research question, has no references to appropriate literature or justification, does not discuss the limitations of the results or how they inform conclusions, or the report does not have a reasonable structure.",
    "crumbs": [
      "Marking Criteria"
    ]
  },
  {
    "objectID": "marking-criteria.html#marks",
    "href": "marking-criteria.html#marks",
    "title": "Marking Criteria",
    "section": "",
    "text": "Marks are awarded in 4 categories, accounting for the following criteria:\n\n\n\nThe selection and effective use of input datasets that are large (e.g.¬†covering multiple years), complex (e.g.¬†containing multiple variables) and/or diverse (e.g.¬†input datasets from multiple sources are used and where appropriate combined in the analysis)\nDescribe how the data was collected and implications for data quality, and outline how the input datasets were downloaded (with a reproducible example if possible), with a description that will allow others to understand the structure of the inputs and how to import them\nEvidence of data cleaning techniques (e.g.¬†by re-categorising variables)\nAdding value to datasets with joins (key-based or spatial), creation of new variables (also known as feature engineering) and reshaping data (e.g.¬†from wide to long format)\n\nDistinction (70%+): The report makes use of a complex (with many columns and rows) and/or multiple input datasets, efficiently importing them and adding value by creating new variables, recategorising, changing data formats/types, and/or reshaping the data. Selected datasets are very well suited to the research questions, clearly described, with links to the source and understanding of how the datasets were generated.\nMerit (60-69%): The report makes some use of complex or multiple input datasets. The selection, description of, cleaning or value-added to the input datasets show skill and care applied to the data processing stage but with some weaknesses. Selected datasets are appropriate for the research questions, with some description or links to the data source.\nPass (50-59%): There is some evidence of care and attention put into the selection, description of or cleaning of the input datasets but little value has been added. The report makes little use of complex or multiple input datasets. The datasets are not appropriate for the research questions, the datasets are not clearly described, or there are no links to the source or understanding of how the datasets were generated, but the data processing aspect of the work acceptable.\nFail (0-49%): The report does not make use of appropriate input datasets and contains very little or now evidence of data cleaning, adding value to the datasets or reshaping the data. While there may be some evidence of data processing, it is of poor quality and/or not appropriate for the research questions.\n\n\n\n\nCreation of figures that are readable and well-described (e.g.¬†with captions and description)\nHigh quality, attractive or advanced techniques (e.g.¬†multi-layered maps or graphs, facets or other advanced techniques)\nUsing visualisation techniques appropriate to the topic and data and interpreting the results correctly (e.g.¬†mentioning potential confounding factors that could account for observed patterns)\nThe report is well-formatted, accessible (e.g.¬†with legible text size and does not contain excessive code in the submitted report) and clearly communicates the data and analysis visually, with appropriate figure captions, cross-references and a consistent style\n\nDistinction (70%+): The report contains high quality, attractive, advanced and meaningful visualisations that are very well-described and interpreted, showing deep understanding of how visualisation can communicate meaning contained within datasets. The report is very well-formatted, accessible and clearly communicates the data and analysis visually.\nMerit (60-69%): The report contains good visualisations that correctly present the data and highlight key patterns. The report is has appropriate formatting.\nPass (50-59%): The report contains basic visualisations or are not well-described or interpreted correctly or the report is poorly formatted, not accessible or does not clearly communicate the data and analysis visually.\nFail (0-49%): The report is of unacceptable quality (would likely be rejected in a professional setting) and/or has poor quality and/or few visualisations, or the visualisations are inappropriate given the data and research questions.\n\n\n\n\nCode quality in the submitted source code, including using consistent style, appropriate packages, and clear comments\nEfficiency, including pre-processing to reduce input datasets (avoiding having to share large datasets in the submission for example) and computationally efficient implementations\nThe report is fully reproducible, including generation of figures. There are links to online resources for others wanting to reproduce the analysis for another area, and links to the input data\n\nDistinction (70%+): The source code underlying the report contains high quality, efficient and reproducible code that is very well-written, using consistent syntax and good style, well-commented and uses appropriate packages. The report is fully reproducible, with links to online resources for others wanting to reproduce the analysis for another area, and links to the input data.\nMerit (60-69%): The code is readable and describes the outputs in the report but lacks quality, either in terms of comments, efficiency or reproducibility.\nPass (50-59%): The source code underlying the report describes the outputs in the report but is not well-commented, not efficient or has very limited levels of reproduicibility, with few links to online resources for others wanting to reproduce the analysis for another area, and few links to the input data.\nFail (0-49%): The report has little to no reproducible, readable or efficient code. A report that includes limited well-described code in the main text or in associated files would be considered at the borderline between a fail and a pass. A report that includes no code would be considered a low fail under this criterion.\n\n\n\n\nTopic selection, including originality, availability of datasets related to the topic and relevance to solving transport planning problems\nClear research question\nAppropriate reference to the academic, policy and/or technical literature and use of the literature to inform the research question and methods\nUse of appropriate data science methods and techniques\nDiscussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed\nDiscuss further research and/or explain the potential impacts of the work\nThe conclusions are supported by the analysis and results\nThe contents of the report fit together logically and support the aims and/or research questions of the report\n\nDistinction (70%+): The report contains a clear research question, appropriate reference to the academic, policy and/or technical literature, use of appropriate data science methods and techniques, discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed. The report discusses further research and/or explores of the potential impacts of the work. Conclusions are supported by the analysis and results, and the contents of the report fit together logically as a cohehisive whole that has a clear direction set-out by the aims and/or research questions. To get a Distinction there should also be evidence of considering the generalisability of the methods and reflections on how it could be built on by others in other areas.\nMerit (60-69%): There is a clear research question. There is some reference to the academic, policy and/or technical literature. The report has a good structure and the results are supported by the analysis. There is some discussion of the strengths and weaknesses of the analysis and input datasets and/or how limitations could be addressed.\nPass (50-59%): The report contains a valid research question but only limited references to appropriate literature or justification. There is evidence of awareness of the limitations of the results and how they inform conclusions, but these are not fully supported by the analysis. The report has a reasonable structure but does not fit together well in a cohesive whole.\nFail (0-49%): The report does not contain a valid research question, has no references to appropriate literature or justification, does not discuss the limitations of the results or how they inform conclusions, or the report does not have a reasonable structure.",
    "crumbs": [
      "Marking Criteria"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "A module teaching how to use data science to solve transport problems.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#general-computing-prerequisites",
    "href": "index.html#general-computing-prerequisites",
    "title": "Transport Data Science",
    "section": "1.1 General computing prerequisites",
    "text": "1.1 General computing prerequisites\nYou should have the latest stable release of R (4.3.0 or above) and be comfortable with computing in general, for example creating folders, moving files, and installing software.\nWe recommend installing this software on a computer with decent resources (e.g.¬†a laptop with 8 GB of more RAM).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#data-science-experience-prerequisites",
    "href": "index.html#data-science-experience-prerequisites",
    "title": "Transport Data Science",
    "section": "1.2 Data science experience prerequisites",
    "text": "1.2 Data science experience prerequisites\nPrior experience of using R or Python (e.g.¬†having used it for work, in previous degrees or having completed an online course) is essential.\nStudents can demonstrate this by showing evidence that they have worked with R before, have completed an online course such as the first 4 sessions in the RStudio Primers series or DataCamp‚Äôs Free Introduction to R course.\nEvidence of substantial programming and data science experience in previous professional or academic work, in languages such as R or Python, also constitutes sufficient pre-requisite knowledge for the course.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "Transport Data Science",
    "section": "1.3 Software",
    "text": "1.3 Software\nAlthough you are free to use any software for the course, the emphasis on reproducibility means that popular popular and established data science languages R and Python are highly recommended.\nThe teaching will be delivered primarily in R, with some Python code snippets and examples. Unless you have a good reason to use Python, we recommend you use R for the course.\n\n1.3.1 R software prerequisites\nFor this module you therefore need to have up-to-date versions of R and RStudio installed on a computer you have access to:\n\nR from cran.r-project.org\nRStudio from rstudio.com\nR packages, which can be installed by opening RStudio and typing install.packages(\"stats19\") in the R console, for example.\nTo install all the dependencies for the module, run the following command in the R console:\n\n\nif (!requireNamespace(\"remotes\", quietly = TRUE)) {\n  install.packages(\"remotes\")\n}\nremotes::install_github(\"itsleeds/tdstests\")\n\nSee Section 1.5 of the online guide Reproducible Road Safety Research with R for instructions on how to install key packages we will use in the module.1\n\n\n1.3.2 Python software prerequisites\nWe installing Python with a modern package manager such as pixi.",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Transport Data Science",
    "section": "Footnotes",
    "text": "Footnotes\n\n\n For further guidance on setting-up your computer to run R and RStudio for spatial data, see these links, we recommend Chapter 2 of Geocomputation with R (the Prerequisites section contains links for installing spatial software on Mac, Linux and Windows): https://geocompr.robinlovelace.net/spatial-class.html and Chapter 2 of the online book Efficient R Programming, particularly sections 2.3 and 2.5, for details on R installation and set-up and the project management section.‚Ü©Ô∏é",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "slides/index.html",
    "href": "slides/index.html",
    "title": "Transport Data Science",
    "section": "",
    "text": "Introduction to Transport Data Science"
  }
]